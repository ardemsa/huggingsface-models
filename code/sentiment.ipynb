{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00008e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arian\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\huggingface-models-13WJgs-Q-py3.11\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fc4538",
   "metadata": {},
   "source": [
    "## 1. UTILIZE FINETUNED MODEL WITH PIPELINE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20034eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "sentiment = pipeline(model = 'cardiffnlp/twitter-roberta-base-sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed777734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_2', 'score': 0.9864782094955444}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment(\"I love my phone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "544d07b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.9747684597969055}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment(\"I hate my phone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076bd90f",
   "metadata": {},
   "source": [
    "## 2. FINETUNE SPECIFIC MODEL FROM SCRATCH:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af3c918c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d6b564c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 4.31k/4.31k [00:00<?, ?B/s]\n",
      "Downloading metadata: 100%|██████████| 2.17k/2.17k [00:00<?, ?B/s]\n",
      "Downloading readme: 100%|██████████| 7.59k/7.59k [00:00<?, ?B/s]\n",
      "Downloading data: 100%|██████████| 84.1M/84.1M [00:39<00:00, 2.12MB/s]  \n",
      "Generating train split: 100%|██████████| 25000/25000 [00:11<00:00, 2148.02 examples/s]\n",
      "Generating test split: 100%|██████████| 25000/25000 [00:11<00:00, 2248.08 examples/s]\n",
      "Generating unsupervised split: 100%|██████████| 50000/50000 [00:12<00:00, 3864.19 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "imdb = load_dataset('imdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec7ba415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42afb22d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"If only to avoid making this type of film in the future. This film is interesting as an experiment but tells no cogent story.<br /><br />One might feel virtuous for sitting thru it because it touches on so many IMPORTANT issues but it does so without any discernable motive. The viewer comes away with no new perspectives (unless one comes up with one while one's mind wanders, as it will invariably do during this pointless film).<br /><br />One might better spend one's time staring out a window at a tree growing.<br /><br />\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb['train']['text'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc9a949c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb['train']['label'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c023e14a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_1', 'score': 0.4160052537918091}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = sentiment(imdb['train']['text'][2])\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd01b0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset = imdb[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "test_subset = imdb[\"test\"].shuffle(seed=42).select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25ce296f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<?, ?B/s]\n",
      "c:\\Users\\arian\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\huggingface-models-13WJgs-Q-py3.11\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\arian\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "config.json: 100%|██████████| 483/483 [00:00<?, ?B/s] \n",
      "vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 1.82MB/s]\n",
      "tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 7.29MB/s]\n",
      "model.safetensors: 100%|██████████| 268M/268M [00:14<00:00, 18.3MB/s] \n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "pretrained_model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(pretrained_model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df973b4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'George P. Cosmatos\\' \"Rambo: First Blood Part II\" is pure wish-fulfillment. The United States clearly didn\\'t win the war in Vietnam. They caused damage to this country beyond the imaginable and this movie continues the fairy story of the oh-so innocent soldiers. The only bad guys were the leaders of the nation, who made this war happen. The character of Rambo is perfect to notice this. He is extremely patriotic, bemoans that US-Americans didn\\'t appreciate and celebrate the achievements of the single soldier, but has nothing but distrust for leading officers and politicians. Like every film that defends the war (e.g. \"We Were Soldiers\") also this one avoids the need to give a comprehensible reason for the engagement in South Asia. And for that matter also the reason for every single US-American soldier that was there. Instead, Rambo gets to take revenge for the wounds of a whole nation. It would have been better to work on how to deal with the memories, rather than suppressing them. \"Do we get to win this time?\" Yes, you do.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_subset['text'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112fc187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['george',\n",
       " 'p',\n",
       " '.',\n",
       " 'co',\n",
       " '##sma',\n",
       " '##tos',\n",
       " \"'\",\n",
       " '\"',\n",
       " 'ram',\n",
       " '##bo',\n",
       " ':',\n",
       " 'first',\n",
       " 'blood',\n",
       " 'part',\n",
       " 'ii',\n",
       " '\"',\n",
       " 'is',\n",
       " 'pure',\n",
       " 'wish',\n",
       " '-',\n",
       " 'fulfillment',\n",
       " '.',\n",
       " 'the',\n",
       " 'united',\n",
       " 'states',\n",
       " 'clearly',\n",
       " 'didn',\n",
       " \"'\",\n",
       " 't',\n",
       " 'win',\n",
       " 'the',\n",
       " 'war',\n",
       " 'in',\n",
       " 'vietnam',\n",
       " '.',\n",
       " 'they',\n",
       " 'caused',\n",
       " 'damage',\n",
       " 'to',\n",
       " 'this',\n",
       " 'country',\n",
       " 'beyond',\n",
       " 'the',\n",
       " 'im',\n",
       " '##agi',\n",
       " '##nable',\n",
       " 'and',\n",
       " 'this',\n",
       " 'movie',\n",
       " 'continues',\n",
       " 'the',\n",
       " 'fairy',\n",
       " 'story',\n",
       " 'of',\n",
       " 'the',\n",
       " 'oh',\n",
       " '-',\n",
       " 'so',\n",
       " 'innocent',\n",
       " 'soldiers',\n",
       " '.',\n",
       " 'the',\n",
       " 'only',\n",
       " 'bad',\n",
       " 'guys',\n",
       " 'were',\n",
       " 'the',\n",
       " 'leaders',\n",
       " 'of',\n",
       " 'the',\n",
       " 'nation',\n",
       " ',',\n",
       " 'who',\n",
       " 'made',\n",
       " 'this',\n",
       " 'war',\n",
       " 'happen',\n",
       " '.',\n",
       " 'the',\n",
       " 'character',\n",
       " 'of',\n",
       " 'ram',\n",
       " '##bo',\n",
       " 'is',\n",
       " 'perfect',\n",
       " 'to',\n",
       " 'notice',\n",
       " 'this',\n",
       " '.',\n",
       " 'he',\n",
       " 'is',\n",
       " 'extremely',\n",
       " 'patriotic',\n",
       " ',',\n",
       " 'be',\n",
       " '##mo',\n",
       " '##ans',\n",
       " 'that',\n",
       " 'us',\n",
       " '-',\n",
       " 'americans',\n",
       " 'didn',\n",
       " \"'\",\n",
       " 't',\n",
       " 'appreciate',\n",
       " 'and',\n",
       " 'celebrate',\n",
       " 'the',\n",
       " 'achievements',\n",
       " 'of',\n",
       " 'the',\n",
       " 'single',\n",
       " 'soldier',\n",
       " ',',\n",
       " 'but',\n",
       " 'has',\n",
       " 'nothing',\n",
       " 'but',\n",
       " 'distrust',\n",
       " 'for',\n",
       " 'leading',\n",
       " 'officers',\n",
       " 'and',\n",
       " 'politicians',\n",
       " '.',\n",
       " 'like',\n",
       " 'every',\n",
       " 'film',\n",
       " 'that',\n",
       " 'defend',\n",
       " '##s',\n",
       " 'the',\n",
       " 'war',\n",
       " '(',\n",
       " 'e',\n",
       " '.',\n",
       " 'g',\n",
       " '.',\n",
       " '\"',\n",
       " 'we',\n",
       " 'were',\n",
       " 'soldiers',\n",
       " '\"',\n",
       " ')',\n",
       " 'also',\n",
       " 'this',\n",
       " 'one',\n",
       " 'avoids',\n",
       " 'the',\n",
       " 'need',\n",
       " 'to',\n",
       " 'give',\n",
       " 'a',\n",
       " 'com',\n",
       " '##pre',\n",
       " '##hen',\n",
       " '##sible',\n",
       " 'reason',\n",
       " 'for',\n",
       " 'the',\n",
       " 'engagement',\n",
       " 'in',\n",
       " 'south',\n",
       " 'asia',\n",
       " '.',\n",
       " 'and',\n",
       " 'for',\n",
       " 'that',\n",
       " 'matter',\n",
       " 'also',\n",
       " 'the',\n",
       " 'reason',\n",
       " 'for',\n",
       " 'every',\n",
       " 'single',\n",
       " 'us',\n",
       " '-',\n",
       " 'american',\n",
       " 'soldier',\n",
       " 'that',\n",
       " 'was',\n",
       " 'there',\n",
       " '.',\n",
       " 'instead',\n",
       " ',',\n",
       " 'ram',\n",
       " '##bo',\n",
       " 'gets',\n",
       " 'to',\n",
       " 'take',\n",
       " 'revenge',\n",
       " 'for',\n",
       " 'the',\n",
       " 'wounds',\n",
       " 'of',\n",
       " 'a',\n",
       " 'whole',\n",
       " 'nation',\n",
       " '.',\n",
       " 'it',\n",
       " 'would',\n",
       " 'have',\n",
       " 'been',\n",
       " 'better',\n",
       " 'to',\n",
       " 'work',\n",
       " 'on',\n",
       " 'how',\n",
       " 'to',\n",
       " 'deal',\n",
       " 'with',\n",
       " 'the',\n",
       " 'memories',\n",
       " ',',\n",
       " 'rather',\n",
       " 'than',\n",
       " 'suppress',\n",
       " '##ing',\n",
       " 'them',\n",
       " '.',\n",
       " '\"',\n",
       " 'do',\n",
       " 'we',\n",
       " 'get',\n",
       " 'to',\n",
       " 'win',\n",
       " 'this',\n",
       " 'time',\n",
       " '?',\n",
       " '\"',\n",
       " 'yes',\n",
       " ',',\n",
       " 'you',\n",
       " 'do',\n",
       " '.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(train_subset['text'][2])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3907f40f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 2577,\n",
       " 1052,\n",
       " 1012,\n",
       " 2522,\n",
       " 26212,\n",
       " 13122,\n",
       " 1005,\n",
       " 1000,\n",
       " 8223,\n",
       " 5092,\n",
       " 1024,\n",
       " 2034,\n",
       " 2668,\n",
       " 2112,\n",
       " 2462,\n",
       " 1000,\n",
       " 2003,\n",
       " 5760,\n",
       " 4299,\n",
       " 1011,\n",
       " 29362,\n",
       " 1012,\n",
       " 1996,\n",
       " 2142,\n",
       " 2163,\n",
       " 4415,\n",
       " 2134,\n",
       " 1005,\n",
       " 1056,\n",
       " 2663,\n",
       " 1996,\n",
       " 2162,\n",
       " 1999,\n",
       " 5148,\n",
       " 1012,\n",
       " 2027,\n",
       " 3303,\n",
       " 4053,\n",
       " 2000,\n",
       " 2023,\n",
       " 2406,\n",
       " 3458,\n",
       " 1996,\n",
       " 10047,\n",
       " 22974,\n",
       " 22966,\n",
       " 1998,\n",
       " 2023,\n",
       " 3185,\n",
       " 4247,\n",
       " 1996,\n",
       " 8867,\n",
       " 2466,\n",
       " 1997,\n",
       " 1996,\n",
       " 2821,\n",
       " 1011,\n",
       " 2061,\n",
       " 7036,\n",
       " 3548,\n",
       " 1012,\n",
       " 1996,\n",
       " 2069,\n",
       " 2919,\n",
       " 4364,\n",
       " 2020,\n",
       " 1996,\n",
       " 4177,\n",
       " 1997,\n",
       " 1996,\n",
       " 3842,\n",
       " 1010,\n",
       " 2040,\n",
       " 2081,\n",
       " 2023,\n",
       " 2162,\n",
       " 4148,\n",
       " 1012,\n",
       " 1996,\n",
       " 2839,\n",
       " 1997,\n",
       " 8223,\n",
       " 5092,\n",
       " 2003,\n",
       " 3819,\n",
       " 2000,\n",
       " 5060,\n",
       " 2023,\n",
       " 1012,\n",
       " 2002,\n",
       " 2003,\n",
       " 5186,\n",
       " 14314,\n",
       " 1010,\n",
       " 2022,\n",
       " 5302,\n",
       " 6962,\n",
       " 2008,\n",
       " 2149,\n",
       " 1011,\n",
       " 4841,\n",
       " 2134,\n",
       " 1005,\n",
       " 1056,\n",
       " 9120,\n",
       " 1998,\n",
       " 8439,\n",
       " 1996,\n",
       " 10106,\n",
       " 1997,\n",
       " 1996,\n",
       " 2309,\n",
       " 5268,\n",
       " 1010,\n",
       " 2021,\n",
       " 2038,\n",
       " 2498,\n",
       " 2021,\n",
       " 29245,\n",
       " 2005,\n",
       " 2877,\n",
       " 3738,\n",
       " 1998,\n",
       " 8801,\n",
       " 1012,\n",
       " 2066,\n",
       " 2296,\n",
       " 2143,\n",
       " 2008,\n",
       " 6985,\n",
       " 2015,\n",
       " 1996,\n",
       " 2162,\n",
       " 1006,\n",
       " 1041,\n",
       " 1012,\n",
       " 1043,\n",
       " 1012,\n",
       " 1000,\n",
       " 2057,\n",
       " 2020,\n",
       " 3548,\n",
       " 1000,\n",
       " 1007,\n",
       " 2036,\n",
       " 2023,\n",
       " 2028,\n",
       " 26777,\n",
       " 1996,\n",
       " 2342,\n",
       " 2000,\n",
       " 2507,\n",
       " 1037,\n",
       " 4012,\n",
       " 28139,\n",
       " 10222,\n",
       " 19307,\n",
       " 3114,\n",
       " 2005,\n",
       " 1996,\n",
       " 8147,\n",
       " 1999,\n",
       " 2148,\n",
       " 4021,\n",
       " 1012,\n",
       " 1998,\n",
       " 2005,\n",
       " 2008,\n",
       " 3043,\n",
       " 2036,\n",
       " 1996,\n",
       " 3114,\n",
       " 2005,\n",
       " 2296,\n",
       " 2309,\n",
       " 2149,\n",
       " 1011,\n",
       " 2137,\n",
       " 5268,\n",
       " 2008,\n",
       " 2001,\n",
       " 2045,\n",
       " 1012,\n",
       " 2612,\n",
       " 1010,\n",
       " 8223,\n",
       " 5092,\n",
       " 4152,\n",
       " 2000,\n",
       " 2202,\n",
       " 7195,\n",
       " 2005,\n",
       " 1996,\n",
       " 8710,\n",
       " 1997,\n",
       " 1037,\n",
       " 2878,\n",
       " 3842,\n",
       " 1012,\n",
       " 2009,\n",
       " 2052,\n",
       " 2031,\n",
       " 2042,\n",
       " 2488,\n",
       " 2000,\n",
       " 2147,\n",
       " 2006,\n",
       " 2129,\n",
       " 2000,\n",
       " 3066,\n",
       " 2007,\n",
       " 1996,\n",
       " 5758,\n",
       " 1010,\n",
       " 2738,\n",
       " 2084,\n",
       " 16081,\n",
       " 2075,\n",
       " 2068,\n",
       " 1012,\n",
       " 1000,\n",
       " 2079,\n",
       " 2057,\n",
       " 2131,\n",
       " 2000,\n",
       " 2663,\n",
       " 2023,\n",
       " 2051,\n",
       " 1029,\n",
       " 1000,\n",
       " 2748,\n",
       " 1010,\n",
       " 2017,\n",
       " 2079,\n",
       " 1012,\n",
       " 102]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(train_subset['text'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dc6d8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'first'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(2034)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687feee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 1045,\n",
       " 2293,\n",
       " 2026,\n",
       " 2564,\n",
       " 1012,\n",
       " 102,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"I love my phone.\", padding='max_length', truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bbb2c6bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] i love my phone. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(\"I love my phone.\", padding='max_length', truncation=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5bcbf5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 3012.79 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 2802.42 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 1330.60 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def process(cases):\n",
    "    #return tokenizer(cases['text'], padding='max_length', truncation=True)\n",
    "    return tokenizer(cases['text'], truncation=True, max_length=10)\n",
    "\n",
    "tokenized_train = train_subset.map(process, batched=True)\n",
    "tokenized_test = test_subset.map(process, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7ccf5cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "676b0baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27e123b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    " \n",
    "def compute_metrics(eval_pred):\n",
    "   load_accuracy = load_metric(\"accuracy\")\n",
    "   load_f1 = load_metric(\"f1\")\n",
    "  \n",
    "   logits, labels = eval_pred\n",
    "   predictions = np.argmax(logits, axis=-1)\n",
    "   accuracy = load_accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "   f1 = load_f1.compute(predictions=predictions, references=labels)[\"f1\"]\n",
    "   return {\"accuracy\": accuracy, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2441b0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./checkpoints\",\n",
    "   learning_rate=2e-5,\n",
    "   per_device_train_batch_size=16,\n",
    "   per_device_eval_batch_size=16,\n",
    "   num_train_epochs=3,\n",
    "   weight_decay=0.01,\n",
    "   save_strategy=\"epoch\",\n",
    "   seed=42\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "   model=model,\n",
    "   args=training_args,\n",
    "   train_dataset=tokenized_train,\n",
    "   eval_dataset=tokenized_test,\n",
    "   tokenizer=tokenizer,\n",
    "   data_collator=data_collator,\n",
    "   compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "542ef70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/189 [00:00<?, ?it/s]You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      " 78%|███████▊  | 147/189 [03:00<00:52,  1.26s/it]"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9371d138",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [00:39,  2.42s/it]                      \n",
      "Downloading builder script: 6.50kB [00:00, 6.49MB/s]                   \n",
      "14it [00:45,  3.25s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.698319673538208,\n",
       " 'eval_accuracy': 0.56,\n",
       " 'eval_f1': 0.5686274509803921,\n",
       " 'eval_runtime': 6.8239,\n",
       " 'eval_samples_per_second': 14.654,\n",
       " 'eval_steps_per_second': 1.026,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941ec68f",
   "metadata": {},
   "source": [
    "Explanation: to reduce training time, we set max length of an input sequence to 10. This is not a good practice, but helps us test the model quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8b955e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59080b47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7796b8b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
